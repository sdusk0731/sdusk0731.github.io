---
layout: single
title: "북스터디 - 대규모 시스템 설계 기초_9장"
categories: 북스터디
tag: [웹 크롤러]
toc: true
toc_sticky: true 
---

# "9장 웹 크롤러 설계"

### 웹 크롤러
> 로봇 또는 스파이더 라고도 부른다.  
> 검색 엔진에서 널리 쓰이는 기술로 웹에 새로 올라오거나 갱신된 콘텐츠를 찾아내는 것이 주된 목적  
> 콘텐츠: 웹 페이지, 이미지, 비디오, PDF파일 등

> 크롤러 용도  
> 검색 엔진 인덱싱: 가장 보편적인 용도로, 웹 페이지를 모아 검색 엔진을 위한 로컬 인덱스를 만든다.  
> ex) Googlebot: 구글 검색 엔진이 사용하는 웹 크롤러

> 웹 아카이빙  
> 나중에 사용할 목적으로 장기보관하기 위해 웹에서 정보를 모으는 절차  
> 국립 도서관이 크롤러를 돌려 웹 사이트를 아카이빙 하는 중  
> ex) 미국 국회 도서관, EU 웹 아카이브

> 웹 마이닝  
> 인터넷에서 유용한 지식을 도출  
> 유명 금융 기업들은 크롤러를 사용해 주주총회 자료나 연차 보고서를 다운받아 기업의 핵심 사업 방향을 알아내기도 한다.  

> 웹 모니터링  
> 인터넷에서 저작권이나 상표권이 침해되는 사례를 모니터링  
> ex) "디지마크"사는 웹 크롤러를 사용해 해적판 저작물을 찾아내서 보고

> 웹 크롤러의 복잡도
> 웹 크롤러가 처리해야 하는 데이터의 규모에 따라 다르다.  
> 설계할 웹 크롤러가 감당해야 하는 데이터의 규모와 기능들을 알아햐 한다.  

> 웹 크롤러 설계
>> ### 1단계. 문제 이해 및 설계 범위 확정  
>>> 웹 크롤러의 기본 알고리즘  
>>> 1\. URL 집합이 입력으로 주어지면, 해당 URL들이 가리키는 모든 웹페이지를 다운로드  
>>> 2\. 다운 받은 웹 페이지에서 URL들을 추출
>>> 3\. 추출된 URL들을 다운로드할 URL 목록에 추가하고 위의 과정을 처음부터 반복  
>
>> ### 좋은 웹 크롤러가 만족시켜야할 속성
>> 1\. 규모 확장성  
>> 웹은 거대하다. 병행성을 활용하면 보다 효과적으로 웹 크롤링을 할수 있다.  
>> 
>> 2\. 안정성
>> 웹은 함정이 가득하다.
>> ex) 잘못된 HTML, 아무 반응 없는 서버, 장애, 악성코드가 붙은 링크 등  
>> 비정상적 입력이나 환경에 잘 대응할 수 있어야 한다.  
>>
>> 3\. 예절  
>> 수집 대상 웹 사이트에 짧은 시간 동안 너무 많은 요청을 보내서는 안된다.  
>>
>> 4\. 확장성  
>> 새로운 형태의 콘텐츠를 지원하기가 쉬워야 한다.  
>
> 개략적 규모 추정  
> 추정치는 많은 가정으로 부터 나오며, 미리 합의해 두는 것이 중요하다.  
>
>> ### 2단계 개략적 설계안 제시 및 동의 구하기
>> \- 요구사항이 분명해지면 개략적 설계를 진행
>>
>> ***1\. 시작 URL 집합***   
>> \- 웹 크롤러가 크롤링을 시작하는 출발점
>>
>> 1\) 모든 웹 페이지를 크롤링하는 가장
직관적인 방법
>> \- 해당 도메인 이름이 붙은 모든 페이지의 URL을 시작 URL로 쓰는 것  
>>
>> 2\) 전체 웹을 크롤링해야 하는 경우  
>> \- 가능한 한 많은 링크를 탐색할 수 있도록 하는 URL을 선택  
>> \- 전체 URL 공간을 작은 부분집합으
로 나누는 전략을 쓴다  
>> 지역적인 특색, 나라별로 인기 있는 웹 사이
트가 다르다는 점 에 착안  
>>
>> \- 주제 별로 다른 시작 URL
을 사용  
>> URL 공간을 쇼핑, 스포츠, 건강 등등의 주제별
로 세분화하고 그 각각에 다른 시작 URL을 사용  
>>
>> ***2\. 미수집 URL 저장소***  
>> \- "다운로드할 URL"을 저장 관리하는 컴포넌트  
>> \- 웹 크롤러는 크롤링 상태를 "다운로드할 URL"과 "다운로드된 URL"의 두가지로 나눠 관리한다.
>>
>> ***3\.HTML 다운로더***  
>> \- 인터넷에서 웹 페이지를 다운로드하는 컴포
넌트  
>> \- 다운로드할 페이지의 URL은 미수집 URL 저장소가 제공
>>
>> ***4\. 도메인 이름 변환기***  
>> \- 웹 페 이지를 다운받으려 면 URL을 IP 주소로 변환하는 절차가 필요  
>> \- 도메인 이름 변환기를 사용하여 URL에 대응되는 IP 주소를 알아낸
다.  
>>
>> ***5\. 콘텐츠 파서***  
>> \- 파싱(parsing)과 검증(validation) 절차를 거쳐야 한다  
>> \- 이상한 웹 페이지는 문제를 일으킬 수 있고 저장 공간만 낭비한다.  
>> 
>> ***6\. 중복 콘텐츠인가>***  
>> \- 자료 구조를 도입하여 데이터 중복을 줄이고 데이터 처리에 소요되는 시간을 줄인다.  
>> \- 웹 페이지의 해시 값을 비교
>>
>> ***7\. 콘텐츠 저장소***  
>> \- HTML문서를 보관하는 시스템
>> \- 데이터 양이 너무 많으므로 대부분의 콘텐츠는 디스크에 저장  
>> \- 인기 있는 콘텐츠는 메모리에 두어 접근 지 연시간을 줄인다.
>>
>> ***8\. URL 추출기***  
>> \- HTML페이지를 파싱하여 링크들을 골라내는 역할
>> \- 상대 경로를 전부 절대 경로로 변환
>>
>> ***9\. URL 필터***  
>> \- 특정 콘텐츠 차입이나 파일 확장자를 갖는 URL  
>> \- 접속 시 오류가 발생하는 URL, 접근 제외 목록에 포함된 URL등을 크롤링 대상에서 배제하는 역할
>>
>> ***10\. 이미 방문한 URL***  
>> \- 이미 방문하거나 미수집 URL저장소에 보관된 URL을 추적할 수 있도록 하는 자료 구조  
>> \- 이미 방문한 적
이 있는 URL인지 추적하면 같은 URL을 여러 번 처리하는 일을 방지할 수 있으
므로 서버 부하를 줄이고 시스템 이 무한 루프에 빠지는 일을 방지  
>> \- 블룸 필터나 해시 테이블이 널리 쓰인다.
>>
>> ***11\. URL 저장소***   
>> \- 이미 방문한 URL을 보관하는 저장소
>
>> ### 웹 크롤러 작업 흐름
>> 1\. 시작 URL들을 미수집 URL 저장소에 저장  
>> 2\. HTML 다운로더는 미수집 URL 저장소에서 URL 목록을 가져온다  
>> 3\. HTML 다운로더는 도메인 이름 변환기를 사용하여 URL의 IP 주소를 알아
내고, 해당 IP 주소로 접속하여 웹 페이지를 다운  
>> 4\. 콘텐츠 파서는 다운된 HTML 페이지를 파싱하여 올바른 형식을 갖춘 페이
지인지 검증  
>> 5\. 콘텐츠 파싱과 검증이 끝나면 중복 콘텐츠인지 확인하는 절차를 개시  
>> 6\. 중복 콘텐츠인지 확인하기 위해서, 해당 페이지가 이미 저장소에 있는지
본다.  
>> \- 이미 저장소에 있는콘텐츠인 경우에는처리하지 않고버린다.  
>> \- 저장소에 없는 콘텐츠인 경우에는 저장소에 저장한 뒤 URL 추출기로 전
달한다
>> 7\. URL 추출기는해당 HTML 페이지에서 링크를 골라낸다.  
>> 8\. 골라낸 링크를 URL 필터로 전달한다.  
>> 9\. 필터 링 이 끝나고 남은 URL만 중복 URL 판별 단계로 전달한다.  
>> 10\. 이미 처리한 URL인지 확인하기 위하여, URL 저장소에 보관된 URL인지 살
핀다. 이미 저장소에 있는 URL은 버 린다.  
>> 11\. 저장소에 없는 URL은 URL 저장소에 저장할 뿐 아니라 미수집 URL 저장소
에도 전달
>
>> ### 3단계 상세 설계
>> ***1\. DFS를 쓸 것인가, BFS를 쓸 것인가***  
>> \- 그래프 탐색에 널리 사용되는
두 가지 알고리즘  
>>> ***DFS(깊이 우선 탐색법)***  
>>> \- 그래프 크기가 클 경우 어느 정도로 깊숙이
가게 될지 가늠하기 어렵다
>>
>>>> ***BFS(너비 우선 탐색법)***  
>>>> \- 웹 크롤러에 보통 쓰이는 알고리즘  
>>>> \- FIFO(First-In-First-Out) 큐를 사용하는 알고리즘  
>>>> \- 큐
의 한쪽으로는 탐색할 URL을 집어넣고, 다른 한쪽으로는 꺼내기만 한다  
>>>
>>>> ***BFS의 두 가지 문제점***  
>>>> 1\. ‘예의 없는(impolite)’ 크롤러  
>>>> \- 한 페이지에서 나오는 링크의 상당수는 같은 서버로 되돌아간다.  
>>>> \- 같은 호스트에 속한 많은 링크를 다운받느라 바빠지게 되는데, 이 때
링크들을 병렬로 처리하게 된다면 서버는 수많은 요청으로 과부하에 걸리게 된다.  
>>>>
>>>> 2\. URL 간에 우선순위를 두지 않는다.  
>>>> 처리 순서에 있어 모든 페이지를 공평하게 대우한다는 뜻이다.  
>>>> 하지만 모든 웹 페이지가 같은 수준의 품질, 같은 수준의 중요성을 갖지는 않는다.  
>>>> 페이지 순위(page rank), 사용자 트래픽의 양, 업데이트 빈도 등 여러 가지 척도에 비추어 처리 우선순위를 구별하는 것이 바람직 하다.
>>
>> ***2\. 미수집 URL 저장소***  
>> \- URL 저장소는 다운로드할 URL을 보관하는 장소  
>> \- 잘 구현하면 ‘예의(politeness)’를 갖춘 크롤러, URL 사이의 우선순위와 신선도
(freshness)를 구별하는 크롤러를 구현할 수 있다.  
>> \- 예의: 웹 크롤러는 수집 대상 서버로 짧은 시간 안에 너무 많은 요청을 보내는 것을
삼가야 한다.  
>> \- 예의 바른 크롤러=동일 웹 사이트에 대해서는 한 번에 한 페이지만 요청  
>> 
>> \- 큐 라우터: 같은 호스트에 속한 URL은 언제나 같은 큐(bl,b2,bn)로 가도록 보장하는 역할  
>> \- 매핑 테이블: 호스트 이름과 큐사이의 관계를 보관하는 테이블  
>> \- FIFO 큐: 같은 호스트에 속한URL은 언제나 같은 큐에 보관  
>> \- 큐 선택기: 큐들을 순회하면서 큐에서 URL을 꺼내서 해당 큐에서 나온 URL을 다운로드하도록 지정된 작업 스레드에 전달하는 역할  
>> \- 작업 스레드: 전달된 URL을 다운로드하는 작업을 수행하며, 전달된 URL은 순차적으로 처리되며, 작업들 사이에는 일정한 지 연시간(delay)을 둘 수 있다.  
>>
>> ***우선순위***  
>> \- 유용성에 따라 URL의 우선순위를 나눌 때는 페이지랭크(PageRank), 트래픽 양, 갱신 빈도(update frequency) 등 다양한 척도를 사용  
>>
>> ***순위결정장치***  
>> \- URL 우선순위를 정하는 컴포넌트  
>> \- 큐: 우선순위별로 큐가 하나씩 할당, 우선순위가 높으면 선택될 확률이 올라감  
>> \- 큐 선택기: 임의 큐에서 처리할 URL을 꺼내는 역할을 담당, 순위가 높은 큐에서 더 자주 꺼내도록 프로그램
>>
>> 전면 큐(front queue)： 우선순위 결정 과정을 처리  
>> 후면 큐(back queue)： 크롤러가 예의 바르게 동작하도록 보증
>>
>> ***신선도***
>> 데이터의 신선함을 유지하기 위해서는 이미 다운로드한 페이지라고 해도 주기적으로 재수집이 필요함  
>> URL을 재수집하는 것은 많은 시간과 자원이 필요한 작업  
>>
>> ***최적화하기 위한 전략***  
>> 웹 페이지의 변경 이력 활용  
>> 우선순위를 활용하여, 중요한 페이지는 좀 더 자주 재수집  
>> 
>>***미수집 URL 저장소를 위한 지속성 저장장치***  
>> 대부분의 URL은 디스크에 두고, IO 비용을 줄이기 위해 메모리 버퍼에 큐를 둔다.  
>> 버퍼에 있는 데이터는 주기적으로 디스크에 기록한다.
>>
>> ***HTML 다운로더***  
>> HTTP 프로토콜을 통해 웹 페이지를 내려 받는다.  
>> 
>> ***Robots.txt***  
>> \- 웹사이트가 크롤러와 소통하는 표준적 방법  
>> \- 크롤러가 수집해도 되는 페이지 목록
>>
>> ***성능 최적화***  
>> \- HTML 다운로더 에 사용할 수 있는 성능 최적화 기법들  
>>
>> 1\) 분산 크롤링  
>> \- 성능을 높이기 위해 크롤링 작업을 여러 서버에 분산
>>
>> 2\) 도메인 이름 변환 결과 캐시  
>> \- DNS 조회 결과로 얻어진 도메인 이름과 IP 주소 사이의 관계를 캐시에 보관해 놓고 크론 잡(cron job) 등을 돌려 주기 적으로 갱신  
>>
>> 3\) 지역성  
>> \- 크롤링 작업을 수행하는 서버를 지역별로 분산하는 방법  
>> \- 크롤링 대상 서버와 지역적으로 가까우면 페이지 다운로드 시간은 줄어들 것
>>
>> 4\) 짧은 타임아웃
>> \- 최대 얼마나 기다릴지를 미리 정해두는 것  
>
>> ***안정성***  
>> \- 최적화된 성능뿐 아니라 안정성도 다운로더 설계 시 중요하게 고려  
>> \- 안정 해시  
>>  \> 다운로더 서버들에 부하를 분산할 때 적용 가능한 기술  
>>
>> \- 크롤링 상태 및 수집 데이터 저장  
>>  \> 장애가 발생한 경우에도 쉽게 복구할 수 있도록 크롤링 상태와 수집된 데이터를 지속적 저장장치에 기록해 두는 것
>>
>> \- 예외 처리  
>> \> 예외가 발생해도 전체 시스템이 중단되는 일 없이 그 작업을 우아하게 이어나갈 수 있어야 한다.  
>>
>> \- 데이터 검증  
>> \> 시스템 오류를 방지하기 위한 중요 수단 가운데 하나
>
>> ***확장성***  
>> \- 시스템을 설계할 때는 새로운 형태의 콘텐츠를 쉽게 지원할 수 있도록 신경 써야 한다.
>
>> ***문제 있는 콘텐츠 감지 및 회피***  
>> \- 중복이거나 의미 없는, 또는 유해한 콘텐츠를 어떻게 감지하고 시스템으로부터 차단  
>>
>> 1\. 중복 콘텐츠  
>> \- 웹 콘텐츠의 30% 가량은 중복  
>> \- 해시나 체크섬(checksum)을 사용하면 중복 콘텐츠를 보다 쉽게 탐지  
>>
>> 2\. 거미 덫  
>> \- 크롤러를 무한 루프에 빠뜨리도록 설계한 웹 페이지  
>> \- URL의 최대 길이를 제한하면 회피  
>> \- 수작업으로 덫을 확인하고 찾아낸 후에 덫이 있는 사이트를 크롤러 탐색 대상에서 제외하거나 URL 필터 목록에 걸어두는 것
>>
>> 3\. 데이터 노이즈  
>> \- 광고나 스크립트 코드, 스팸 URL 같은 것들은 거의 가치가 없다.  
>> \- 도움될 것이 없으므로 가능하다면 제외  
>
>> ### 4단계 마무리  
>> ***원치 않는 페이지 필터링***  
>> 크롤링에 소요되는 자원은 유한하기 때문에 스팸 방지 (anti-spam) 컴포넌트를 두어 품질이 조악하거나 스팸성 페이지를 걸러내도록 해 두면 좋다.
>>
>> ***데이터베이스 다중화 및 샤딩***  
>> 다중화(replication)나 샤딩(sharding) 같은 기법을 적용하면 데이터 계층(data layer)의 가용성, 규모 확장성, 안정성 향상
>>
>> ***수평적 규모 확장성***  
>> 수평적 규모 확장성을 달성하는 데 중요한 것은 서버가 상태정보를 유지하지 않도록 하는 것, 즉 무상태(stateless) 서버로 만드는 것
>>
>> ***가용성, 일관성, 안정성***  
>> 성공적인 대형 시스템을 만들기 위해 필수적으로 고려해야 하는 것들
>>
>> ***데이터 분석 솔루션***  
>> 데이터를 수집하고 분석하는 것은 어느 시스템에게나 중요